# The case for altmetrics

There are a multitude of reasons for believing that altmetrics offer a viable option when measuring scholarly impact. The capability of altmetrics to measure more diverse forms of impact, with less delay, more transparently and at enormous scales is exciting for those looking to improve scholarly impact measurement. Priem & Hemminger (2010) find that "altmetrics take advantage of the pervasiveness and importance of new tools (Web pages, search engines, e–journals) to inform broader, faster, and more open metrics of impact". New data sources allow altmetrics practitioners to explore the underlying properties of an article to "measure the distinct concept of social impact" (Eysenbach; 2011).

### Correlation with citation

Many altmetrics studies have focussed on correlating various data sources with traditional citations as a way of validating their results. Thelwall, Haustein, Lariviere et al. (2013) attempted to correlate 11 different data sources with Web of Science citation counts. They created a simple sign test whether an article's citations match altmetrics values - if both were higher than the average for articles published at a similar time then the data source was considered successful. They found "clear evidence" that data sources for blogs, Facebook and Twitter show strong correlation with citations - "the success rate of the altmetrics at associating with higher citation significantly exceeded the failure rate at the individual article level". They found that an additional three data sources correlate with citations, although the correlation is weaker.

Another study found a "moderately strong relationships between citation count and pdf/html download count", and that "Mendeley and CiteULike displayed the highest correlations to Web of Science counts". From their original sample, they created a time-restricted sample of recent (2011) papers, finding that correlations between Mendeley and Web of Science citation count "rivaled or surpassed those of Scopus, PubMed, and CrossRef citations" (Priem, Piwowar, Hemminger, 2012).

Some studies focussed specifically on a single data source. Nielsen (2007) finds that the number of links to research papers in Wikipedia correlates with the citation count found in the Journal Citation Report (from which the impact factor is calculated). Similarly, Eysenbach (2011) finds a statistically significant, although weak, correlation between Twitter citations and traditional citation counts.

### Capturing the conversations

Altmetrics have the potential to capture previously hidden scholarly communications. Conversations between academics in the hallways at their institutions or at conferences are important methods for discussing new and interesting ideas. These discussions have huge value in determining the course of future science, and thus are valuable sources of impact. As academia increasingly moves online, it seems logical that these conversations will also move online. Altmetrics gives us the opportunity to measure this impact.

Priem, Piwowar & Hemminger (2012) claim that "[Blogging and Twitter] facilitate the sort of informal conference chats that have long vivified the academy’s invisible colleges". They also point out that this form of impact measurement "[facilitates] existing practice", instead of forcing new behaviour.

### Diversity

One of the biggest benefits of altmetrics is impact discovery from a much broader and diverse set of data sources. Fenner (2013) describes scientific impact as "multi-dimensional construct that can not be adequately measured by any single indicator", supporting altmetric's model of many diverse data sources. By using a wider variety of sources, the impact reflects the real-world "diverse scholarly ecosystem" more accurately ("altmetrics: a manifesto", 2010). Gibson (2013) examined the altmetric data sources used by the Journal of Ecology finding that "each metric reflected a different form of reader usage".

As discussed in the History of Bibliometrics section, the Impact Factor fails to measure the general public's interaction with science. Altmetrics, due to their inherent diversity, allow us to measure this interaction. This can be achieved by data from sources that are used by people outside of science, such as social networks (Priem, Piwowar & Hemminger, 2012). In addition, other under-represented or niche groups can be looked at to discover impact. Thelwell, Haustein & Lariviere (2013) studied the validity of altmetrics using Twitter as a primary data source, by attempting to correlate Twitter metrics with citation counts. They comment that this may be limiting to the scope of altmetrics, where it can capture the "influence of scholarly publications on a wider and different section of their readership than citation counts".

The benefits of this wider group extend to other areas of scholarship, such humanities and social science. As discussed in the History of Bibliometrics section, these fields miss out on the impact factor as they rely on alternative forms of publication to the journal. Altmetrics gives us the opportunity to fix this imbalance (Roemer & Borchardt, 2013).

The range of altmetrics can be adapted to multiple different purposes and contexts (Neylon & Wu, 2009), allowing us to view impact in novel ways. For example, Priem, Piwowar & Hemminger (2012) propose a set of "impact flavours", that can be used to describe the impact of papers. This concept and similar have evolved to the categories of data sources discussed in the previous section. Using these categorisations, and applying them to papers - i.e. papers that have relatively high metrics from one category are, by implication, similar to other papers with high metrics in the category - we can view the impact through the lens of these categories. This allows us to look at metrics more nuanced and encompassing way. As Priem, Piwowar & Hemminger (2012) note, "the goal is not to compare flavors: one flavor is not objectively better than another", which leads to the recognition that "different types of contributions might help us appreciate scholarly products for the particular needs they meet".

Altmetrics can also be used to generate a more semantic view of influence, by looking at the entire system of sharing and re-use ("altmetrics: a manifesto", 2010). Google's PageRank system famously ranks pages on the web by considering which other web pages link to the page in question. Links from trusted sites hold more weight, therefore a page containing a link from these sites is given a higher ranking. This concept can be applied to altmetrics - by considering who or what cites a scholarly article, a weighting can be applied to that citation that will be reflected the article's impact. Bollen, Van de Sompel, Hagberg et al. (2009) studied this concept using a "clickstream" model to investigate how scientific publications are accessed. A map of how users clicked to move through the system was collected, and modelled. They drew few conclusions on it's validity as a influence measure, although it was found that "[there is a] distinction between citing behaviour and online information seeking behaviour", implying that altmetrics can discover impact that is unseen by traditional citation metrics.

### Speed

As discussed in the Failings of Traditional Metrics section, citation analysis can take several months if not years to accumulate. Altmetrics analysis, on the other hand, can gather data in "days or weeks" ("altmetrics: a manifesto", 2010). By leveraging the power of open APIs, altmetric data sources can be queried immediately after a paper has been published (Chamberlain, 2013). For example, tweets usually occur very soon after publication, almost as the paper is read and shared by other academics. Priem & Light Costello (2011) find that, for Twitter citations "39% ... refer to articles less than one week old, and 15% of citing tweets refer to articles published that same day". Groth & Gurney (2010) comment that one of blogging's "major strengths" is "the ability to provide instantaneous commentary on a subject with simultaneous feedback on their own content". They also comment that blogs allow discussion of older papers, putting current papers into context and reintroducing older ideas.

This much shorter delay between publication and citation presents a great opportunity to altmetrics practitioners, enabling novel uses for altmetrics data. The Altmetrics manifesto (2010) claims that it allows the opportunity to develop "real-time recommendation and collaborative filtering systems". This would add an additional layer to existing review systems within science, cutting the overwhelming and increasing volume of papers published yearly, and allowing researchers to focus on the most important papers in their field.

* Transparency/openness
	* "They’re open – not just the data, but the scripts and algorithms that collect and interpret it" (Altmetrics manifesto)
	* "Openness ... if data sources are open, conclusions based on article-level metrics can be verified by others and tools can be built on top of the article-level metrics" (Consuming Article-Level Metrics: Observations and Lessons; Chamberlain; 2013)

* Captures all/more of author's work
	* "Sharing information is a central component of [researchers] work" (How and why scholars cite on Twitter; Priem & Light Costello; 2011)

* Complimentary to traditional metrics
	* "usage-based metrics are increasingly perceived by the scientific community as a necessary complement to traditional peer review as an indicator of scientific significance" (Harnad, cited by McKiernan; 2004)
	"Usage-based metrics, as a way to complement traditional peer review, are perceived as a major need by several actors (authors, librarians, publishers) in the scientific communication system" (Harnad, cited by McKiernan; 2004)
	* "The future, then, could see altmetrics and traditional bibliometrics presented together as complementary tools presenting a nuanced, multidimensional view of multiple research impacts at multiple time scales" (Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012)
* At large scale
	* "Taken at the individual level, these metadata are hardly of any interest, but at a large scale metrics based on these metadata are likely to outperform more traditional evaluation processes in terms of coverage, speed and efficiency" (Soft Peer Review; Taraborelli; 2008)
	* Neylon & Wu (2009) compare usage statistics (download numbers) to the advertising industry
		* "[They] may not be completely accurate but they are consistent, comparable, and considered sufficiently immune to cheating to be the basis for a billion dollar Web advertising industry"
* Conclusion
	* Ultimately peer review & impact factors are merely filters for trusted information
		* Take all information, filter it out in various ways to get information that is trustworthy and valuable
		* Altmetrics offer an alternative filter, that can give a different perspective on what information is trustworthy & valuable
		* Should traditional methods be replaced? No - they still offer a useful filter (especially for trustworthy information)
			* Hierarchy of information
		* "Our only options are to publish less or to filter more effectively, and any response that favours publishing less doesn't make sense, either logistically, financially, or ethically" (Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009)
		* Mention Clay Shirky's "Not Information Overload, But Filter Failure"?
	* "Ultimately, users must frame appropriate questions and decide what information they want the altmetrics data to provide"
	* Not a quote, but would be a good comment -> Social media scholarly usage is low currently, but is likely to increase