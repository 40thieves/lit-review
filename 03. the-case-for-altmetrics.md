# The case for altmetrics

There are a multitude of reasons for believing that altmetrics offer a viable option when measuring scholarly impact. The capability of altmetrics to measure more diverse forms of impact, with less delay, more transparently and at enormous scales is exciting for those looking to improve scholarly impact measurement. Priem & Hemminger (2010) find that "altmetrics take advantage of the pervasiveness and importance of new tools (Web pages, search engines, e–journals) to inform broader, faster, and more open metrics of impact". New data sources allow altmetrics practitioners to explore the underlying properties of an article to "measure the distinct concept of social impact" (Eysenbach; 2011).

### Correlation with citation

Many altmetrics studies have focussed on correlating various data sources with traditional citations as a way of validating their results. Thelwall, Haustein, Lariviere et al. (2013) attempted to correlate 11 different data sources with Web of Science citation counts. They created a simple sign test whether an article's citations match altmetrics values - if both were higher than the average for articles published at a similar time then the data source was considered successful. They found "clear evidence" that data sources for blogs, Facebook and Twitter show strong correlation with citations - "the success rate of the altmetrics at associating with higher citation significantly exceeded the failure rate at the individual article level". They found that an additional three data sources correlate with citations, although the correlation is weaker.

Another study found a "moderately strong relationships between citation count and pdf/html download count", and that "Mendeley and CiteULike displayed the highest correlations to Web of Science counts". From their original sample, they created a time-restricted sample of recent (2011) papers, finding that correlations between Mendeley and Web of Science citation count "rivaled or surpassed those of Scopus, PubMed, and CrossRef citations" (Priem, Piwowar, Hemminger, 2012).

Some studies focussed specifically on a single data source. Nielsen (2007) finds that the number of links to research papers in Wikipedia correlates with the citation count found in the Journal Citation Report (from which the impact factor is calculated). Similarly, Eysenbach (2011) finds a statistically significant, although weak, correlation between Twitter citations and traditional citation counts.

* Capturing the un-captured conversations
	* "[Blogging and Twitter] facilitate the sort of informal conference chats that have long vivified the academy’s invisible colleges" (Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012)
		* They "facilitate existing practice" - moving the conversation online - matches with the 'growing use of social media' ideas
* Diversity
	* "Scientific impact is a multi-dimensional construct that can not be adequately measured by any single indicator"
	* Clickstream model
		* Map of clicks as users move through system
		* Shows how impact cannot be simplified to a single number
			* "Look beyond counting and emphasise _semantic_ content like usernames, timestamps, and tags" (Altmetrics manifesto)
	* Using more sources, that attract a different (wider) variety of users
		* Impact that reflects the real-world better
		* A more complete view of impact
	* "Altmetrics are themselves _diverse_, they’re great for measuring impact in this diverse scholarly ecosystem" (Altmetrics manifesto)
	* "The [alt]metrics are also useful in different contexts" (Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009)
	* "Twitter citations are much faster than traditional citations, with 40% occurring within one week of the cited resource’s publication" (How and why scholars cite on Twitter; Priem & Light Costello; 2011)
	* Thelwall, Haustein & Lariviere attempt to correlate altmetrics with citations, but note that this may be limiting to the scope of altmetrics. "Might be able to capture influence of scholarly publications on a wider and different section of their readership than citation counts". (Do altmetrics work? Twitter and ten other social web services; Thelwall, Haustein, Lariviere et al; 2013)
	* "The clusters of impact patterns could be considered the 'impact flavor' of the research article" (Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012)
		* These flavours of impact are more valuable than citations alone "cannot rival"
		* "The goal is not to compare flavors: one flavor is not objectively better than another"
			* One type of impact is not better than another
		* "Recognizing different types of contributions might help us appreciate scholarly products for the particular needs they meet"
		* "[Altmetrics] and citations track forms of impact that are distinct, but related; neither approach is able to describe the complete picture of scholarly use alone"
		* "Many altmetric indicators seem mostly orthogonal to citation"
			* i.e. not parallel - show different forms of impact
	* Social sciences/humanities tend not to emphasise journals - miss out on impact factor (Institutional Altmetrics and Academic Libraries; Roemer & Borchardt; 2013)
		* "Consequently, faculty in the humanities and social sciences have predominantly based their impact narratives on qualitative indicators"
		* Possible that altmetrics might offer quantative measures, at least in terms of wider (social) impact
	* "Magazine sections, ... “add value” to the research articles by interpreting them for a wider audience" (The Impact Factor Game; PLoS Medicine Editors; 2006)
		* Something altmetrics can help to measure
	* Uptake of Journal of Ecology papers in 2012: A comparison of metrics; Gibson; 2013
		* When comparing impact factor, citation count and altmetrics, they found that "each metric reflected a different form of reader usage"
* Speed
	* "Altmetrics are fast, using public APIs to gather data in days or weeks" (Altmetrics manifesto)
	* Data can be gathered now, unlike citation counts
		* The fact that we have evidence already (only a few years after the idea was proposed) backs this up
	* "The speed of altmetrics presents the opportunity to create real-time recommendation and collaborative filtering systems" (Altmetrics manifesto)
		* Enabling new uses for usage data
	* "Scientific blogs (and all other types of blogs) have major strengths, in that they have the ability to provide instantaneous commentary on a subject with simultaneous feedback on their own content" (Studying Scientific Discourse on the Web Using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010)
		* But on the other hand, they allow discussion of older papers: "Scientific blogs also have the ability to reintroduce older publications to the reader environment, where the scientific content may be out of the scope of interest of current research spheres but may still hold interest amongst involved readers" (Also Groth & Gurney)
	* "Speed ... article-level metrics are nearly real-time metrics of scholarly impact" (Consuming Article-Level Metrics: Observations and Lessons; Chamberlain; 2013)
* Transparency/openness
	* "They’re open – not just the data, but the scripts and algorithms that collect and interpret it" (Altmetrics manifesto)
	* "Openness ... if data sources are open, conclusions based on article-level metrics can be verified by others and tools can be built on top of the article-level metrics" (Consuming Article-Level Metrics: Observations and Lessons; Chamberlain; 2013)
* Captures all/more of author's work
	* "Sharing information is a central component of [researchers] work" (How and why scholars cite on Twitter; Priem & Light Costello; 2011)
* Complimentary to traditional metrics
	* "usage-based metrics are increasingly perceived by the scientific community as a necessary complement to traditional peer review as an indicator of scientific significance" (Harnad, cited by McKiernan; 2004)
	"Usage-based metrics, as a way to complement traditional peer review, are perceived as a major need by several actors (authors, librarians, publishers) in the scientific communication system" (Harnad, cited by McKiernan; 2004)
	* "The future, then, could see altmetrics and traditional bibliometrics presented together as complementary tools presenting a nuanced, multidimensional view of multiple research impacts at multiple time scales" (Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012)
* At large scale
	* "Taken at the individual level, these metadata are hardly of any interest, but at a large scale metrics based on these metadata are likely to outperform more traditional evaluation processes in terms of coverage, speed and efficiency" (Soft Peer Review; Taraborelli; 2008)
	* Neylon & Wu (2009) compare usage statistics (download numbers) to the advertising industry
		* "[They] may not be completely accurate but they are consistent, comparable, and considered sufficiently immune to cheating to be the basis for a billion dollar Web advertising industry"
* Conclusion
	* Ultimately peer review & impact factors are merely filters for trusted information
		* Take all information, filter it out in various ways to get information that is trustworthy and valuable
		* Altmetrics offer an alternative filter, that can give a different perspective on what information is trustworthy & valuable
		* Should traditional methods be replaced? No - they still offer a useful filter (especially for trustworthy information)
			* Hierarchy of information
		* "Our only options are to publish less or to filter more effectively, and any response that favours publishing less doesn't make sense, either logistically, financially, or ethically" (Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009)
		* Mention Clay Shirky's "Not Information Overload, But Filter Failure"?
	* "Ultimately, users must frame appropriate questions and decide what information they want the altmetrics data to provide"
	* Not a quote, but would be a good comment -> Social media scholarly usage is low currently, but is likely to increase