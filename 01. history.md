# History of bibliometrics

The field of bibliometrics, sometimes scientometrics, has existed for many years and has created a set of methods to quantitatively analyse scientific and technological literature [Bibliometrics and Citation Analysis: From the Science Citation Index to Cybermetrics; De Bellis; 2009]. These metrics are most commonly used to measure the impact, or value, of the research in question. Fenner defines impact as follows: "The scientific impact of a particular piece of research is reflected in how this work is taken up by the scientific community" [What Can Article-Level Metrics Do for You?; Fenner; 2013]. This impact ranking has a diverse set of applications, including assessment of an author's work. One of the most widely used methods is the Impact Factor, sometimes known as the Journal Impact Factor.  It was first proposed by Eugene Garfield in 1972, in his paper, _Citation Analysis As A Tool in Journal Evaluation_. The impact factor is calculated using the following algorithm:

![Impact Factor algorithm](img/impact-factor.png)

The _Journal Citation Reports_ is published annually by Thomson Reuters, listing all known journals and giving their impact factor, and other metrics for the current year. Neylon and Wu [Article-Level Metrics and the Evolution of Scientific Impact; Neylon & Wu, 2009] state that "most scientists … will point to the Thomson ISI Journal Impact Factor as an external and 'objective' measure for ranking the impact of specific journals and the individual articles within them".

Usage of the impact factor in ranking research other than journals has become more widespread. Increasingly, impact factor has become a proxy for measuring many diverse research outputs. These range from comparisons of international impact to individual article value [Cash for papers: putting a premium on publication; Fuyuno & Cyranoski; 2006]  [Nefarious Numbers; Arnold & Fowler; 2010]  [The History and Meaning of the Journal Impact Factor; Garfield; 2006]. Article value is calculated by proxy, by simply taking the impact factor from the journal it was published in. This has lead to ranking author value, by totalling the impact factor of each paper published.

Because of it's wide-ranging use, the impact factor has a strong influence on the scientific community. This has affected decisions on where to publish, whom to promote or hire, the success of grant applications, library decisions to purchase and renew journal subscriptions, researchers deciding where to publish, researchers choice on what to read and even salary bonuses [Show Me The Data; Rossner, Van Epps & Hill; 2007]  [Nefarious Numbers; Arnold & Fowler; 2010]. The Public Library of Science Medicine (PLoS Medicine) Editors report that "in some countries, government funding of entire institutions is dependent on the number of publications in journals with high impact factors" [It is time to find a better way to assess the scientific literature; The PLoS Medicine Editors; 2006]. 

In the UK, governmental assessment of Higher Education institutions have been conducted by the Research Assessment Exercise (RAE) since 1986. The exercise relied on the "subjective assessment of scientific publications by a panel of experts". Because of this, the RAE was time-consuming and expensive, costing the UK Government £12 million and universities an additional £47 million [The Assessment of Science: The Relative Merits of Post-Publication Review, the Impact Factor, and the Number of Citations; Eyre-Walker & Stoletzki; 2013]. In 2014, the RAE will be replaced by the Research Excellence Framework (REF). The REF will controversially provide more focus research impact, with 25% of the final grading going towards measurement of value [Humanities research threatened by demands for 'economic impact'; Shepherd; 2009]. Allen, Jones and Dolby believe that it is the impact factor's place as the key indicator of research progression that provides much of the rationale for the move to a more metrics-based successor [Looking for landmarks: the role of expert review and bibliometric analysis in evaluating scientific publication outputs; Allen, Jones & Dolby; 2009].

* h-index & other metrics (?)
* Historical use cases (?)
	* Live peer review
	* Filtering (linked to above)
	* Performance review
		* Funding
		* Tenure
* Forms of impact (?)
* Define "research output"
	* Articles/papers
	* Blogs/Tweets

# Failings of traditional metrics

The impact factor has been the subject of much criticism, with many papers reporting on its faults. Increasingly, scientists have been calling for an end to the wide-ranging use of the impact factor. In December 2012, researchers at the annual meeting of the American Society for Cell Biology signed the Declaration on Research Assessment, calling for the end of the use of journal metrics to assess individual articles or authors.

Arnold & Fowler report that "the allure of the impact factor as a single, readily available number - not requiring complex judgments or expert input, but purporting to represent journal quality - has proven irresistible to many". It is this inherent simplicity that lead to the rise of the impact factor. As we shall see, this simplicity fails to interpret the scope and complexity of scientific impact. 

As explained above, the impact factor has been used as a proxy for other forms of impact - primarily article or author impact. This is problematic, as the impact factor was simply not designed to measure anything other than journal impact [Article Level Metrics and the Evolution of Scientific Impact]. King & Tenopir find that only about 15 - 20% of scientists in the United States have authored a refereed article, further supporting the view that journal impact is not representative of author impact. Even Garfield, who proposed the original impact factor, has criticised this usage. In his paper, _How to Use Citation Analysis for Faculty Evaluations, and When Is It Relevant?_, he states that citation analysis can augment author assessment, but find that it is easily misinterpreted or inadvertently manipulated [How to Use Citation Analysis for Faculty Evaluations, and When Is It Relevant?; Garfield; 1983]. Additionally, the current publishers of the impact factor, Thomson Reuters have admitted that it is being used in "many inappropriate ways" [It is time to find a better way to assess the scientific literature; The PLoS Medicine Editors; 2006]. 

Scientific publishing has grown to an incredible rate, with over 800 000 papers published in 2008 in PubMed alone [Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009]. This has grown to nearly 950 000 papers in 2013 [2013 PubMed Search Results; nd]. Neylon and Wu claim this growth has overwhelming for researchers - "It [is] impossible for any scientist to read every paper relevant to their research, and a difficult choice has to be made about which papers to read" [Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009]. Renear and Palmer find that scientists read 50% more papers than they did in the 1970s, spending less time on average with each one [Strategic Reading, Ontologies, and the Future of Scientific Publishing; Renear and Palmer, 2009]. Mendeley, a research paper bookmarking service found that researchers spent an average of 1:12 hours per day studying literature [Global Research Report; 2009]. Choices where researcher's time reading papers now must be made, leading to a greater need for filtering the mass of papers. Traditional forms of filtering, namely, peer review and the impact factor, are overwhelmed by the scale of modern research [Altmetrics Manifesto; Priem, Taraborelli, Groth &  Neylon; 2010].

The impact factor relies on citations as it's base measurement, which take time to accumulate. The average paper is not cited for months, at the earliest, but more often until 1 - 2 years after publication [Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009]  [Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012]. Potential impact, therefore, takes a long time to accumulate, especially relative to the rapid pace of development. This means that decisions using citation analysis data made soon after publication, may be incomplete, affecting career decisions, decisions on where to publish, funding decisions, and many other decisions. Additionally, real-time ranking of output becomes very difficult, preventing possible filtering applications.

* Also affects other citation-based metrics (h-index?)

Researchers have also pointed out the underlying mathematical issues with the impact factors. In a _Journal of Cell Biology_ Editorial, Rossner, Van Epps and Hill report that 89% of _Nature's_ citations are attributable to only 25% of papers. Fundamentally, the impact factor is a mean, so it can be "badly skewed by a 'blockbuster' paper" [Show Me The Data; Rossner, Van Epps & Hill; 2007].

* Only 15% to 20% of scientists in the US have authored a refereed article

Allen, Jones & Dolby compared expert reviews of research articles to the impact factor, and other citation-based metrics. They found that the expert's score was more strongly correlated to the impact factor than to the number of citations the paper had received. They believe this was a consequence of experts rating papers in high profile (and high impact) journals more highly, rather than an ability of experts to judge the intrinsic metric or likely impact of a paper [Looking for landmarks: the role of expert review and bibliometric analysis in evaluating scientific publication outputs; Allen, Jones & Dolby; 2009]. This presents the current metric system with a "rich get richer" problem - papers published in journals with existing high impact factors are perceived to be "better", leading to further high impact factors. This means it is difficult for papers in new journals to break into the system. A metric that considers more diverse communication methods might address this issue.

Measurement of scientific value have attempted to find metrics that measure scientific value outside of traditional scholarly communication for some time [Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012]. The impact factor only measures impact within the scientific community, through it's sole use of citations. Any attempt by researchers to provide outreach to the public is ignored by the impact factor [It is time to find a better way to assess the scientific literature; The PLoS Medicine Editors; 2006]. Citation is evolving, and the published scientific article is not always the primary method of communicating scientific thinking [The Impact Factor: A Tool from a Bygone Era? Anderson; 2009]. In the current research climate, where the impact factor has power over career paths, outreach is discouraged. The PLoS Medicine Editors write that "[For a journal] which strives to make ... open-access content reach the widest possible audience ... impact factor is a poor measure of overall impact". They believe that by reaching a wider audience, they can help to set agendas - by publishing policy papers or highlighting neglected issues [It is time to find a better way to assess the scientific literature; The PLoS Medicine Editors; 2006].

Other areas of scholarly work are also overlooked by the impact factor. The published paper is not necessarily the total output of a researcher, although it is often used to summarise. The Altmetrics manifesto authors report that there are "new forms [of communication that] reflect and transmit scholarly impact" [Altmetrics Manifesto; Priem, Taraborelli, Groth &  Neylon; 2010]. Additional work might include data sets, or code, that may go on to be reused by further research, increasing it's impact [Data reuse and the open data citation advantage; Piwowar & Vision; 2013]. However this potential impact will be not be captured by the impact factor.

Additionally, some forms of scholarly communication that were rarely recorded before are now becoming commonplace in digital formats [Studying Scientific Discourse on the Web Using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010]  [How and why scholars cite on Twitter]  [How and why people Twitter: the role that micro-blogging plays in informal communication at work; Zhao and Rosson; 2009]  [Understanding how Twitter is used to spread scientific messages; Letierce; 2009]  [Research blogs and the discussion of scholarly information; Shema, Bar-Ilan, Thelwall; 2012]. Researchers who collaborate well, or can explain concepts well have unpublished, but still valuable impact. Priem, Piwowar & Hemminger call this "scientific street cred", the informal and sometimes unintentional credit that scholars receive from peers that is moving online, where it can be tracked and quantified [Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012]. Again, the impact factor does not take this hidden impact into consideration, by it's citation-based nature.

Authors have criticised the impact factor's simplicity, in that it is merely a straight count of citations, and does not take the effect of a network of researchers into account. Bollen, Rodriquez & Van de Sompel write that it "only represents the popularity factor of status, not its prestige factor", where _popularity_ is defined as the total number of endorsements, and _prestige_ is the relative prestige of endorsing actors. In their proposed system, a citation from a paper with higher prestige would be weighted more than a citation from a paper with lower prestige. This system is similar to Google's very successful PageRank algorithm, which allowed large scale ranking of web pages to produce accurate and relevant search engine. Bollen, Rodriquez & Van de Sompel find that their PageRank-like algorithm "strongly-overlapped" with the impact factor, but it "revealed significant and meaningful discrepancies" [Journal status; Bollen, Rodriquez & Van de Sompel; 2006]. The simplicity of the impact factor does not allow for the concept of network effects, reducing it's usefulness as a tool for filtering.

* Method is proprietary & trade secret
	* Esp. bad considering it's influence over employment - should be open
	* Evidence that Thomson Reuters have misidentified article types - affects IF
		* "The total number of citations for each journal was substantially fewer than the number published on the JCR website"
	* Wouldn't provide data used to calculate IF
		* Primary data unavailable, it wouldn't be accepted as a paper - _unscientific_
	* "It became clear that the process of determining a journal's impact factor is unscientific and arbitrary"
	* It is not clear whether Thomson Scientific could measure ... individual article citations accurately"
	* "The final calculations for impact factors are largely unknown, and the underlying data are not subject to independent audit"
	* IF is not reproducible
* Citations to retracted papers still count towards the IF
	* Example: "Woo Suk Hwang's stem cell papers in Science from 2004 and 2005, both subsequently retracted, have been cited a total of 419 times"
* Problems with gaming
	* Publishing papers that garner many citations
		* Review articles (articles that review the journal's output - i.e. cites the journal a lot), data heavy articles
	* "There have also been widespread complaints by authors of manuscripts under review, who were asked or required by editors to cite other papers from the journal"
		* "This practice borders on extortion"
* Encourages bad scientific practices
	* "The current promotion system, however, discourages publishing research with negative results"
		* Papers with negative results don't get many citations
		* Therefore there is no incentive
	* [Are Alternative Metrics Still Alternative?; Buschman & Michalek; 2013] find that blogs are increasingly used to publish negative results, methods
* Terms of use restrictions (limits of webometrics)
	* "Webometric approaches that rely on search engines are fundamentally limited by terms of use restrictions on automated mining of results"
	* "Global usage data are generally wrapped in mystery by the publishers"
* Impact factor not good at assessing some subjects
	* Humanities
	* Subjects that are less focussed on articles/papers suffer with IF
* Criticism of the REF
	* "Both subjective review and the number of citations are very error prone measures of merit, so it seems likely that [the REF] will also be extremely error prone, particularly given the volume of assessments that need to be made"
	* Criticism of all metric based measurement
		* In an ideal world, all assessment would be done subjectively
		* However there is too much research for this to be feasible
		* Metrics should be a tool to assist subjective assessment