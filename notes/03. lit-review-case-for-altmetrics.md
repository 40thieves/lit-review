# Final Year Project
### Lit Review - The case for altmetrics

* Ultimately peer review & impact factors are merely filters for trusted information
	* Take all information, filter it out in various ways to get information that is trustworthy and valuable
	* Altmetrics offer a alternative filter, that can give a different perspective on what information is trustworthy valuable
	* Should traditional methods be replaced? No - they still offer a useful filter (especially for trustworthy information)

* [Altmetrics Manifesto](http://altmetrics.org/manifesto/)
	* "Altmetrics are themselves _diverse_, they’re great for measuring impact in this diverse scholarly ecosystem"
	* "Altmetrics are fast, using public APIs to gather data in days or weeks"
	* "They’re open–not just the data, but the scripts and algorithms that collect and interpret it"
	* "Look beyond counting and emphasise _semantic_ content like usernames, timestamps, and tags"
	* "With altmetrics, we can crowdsource peer-review"
	* "In the short term, this is likely to supplement traditional peer-review"
	* "In the future, greater participation and better systems for identifying expert contributors may allow peer review to be performed entirely from altmetrics"
	* Diversity of approaches leads to a more complete view of impact
![Four ways to measure impact](img/four-ways-measure-impact.png)
	* "The speed of altmetrics presents the opportunity to create real-time recommendation and collaborative filtering systems"
* [Measuring Impact Beyond Academic Fame: An Alternative Social Impact Factor](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8330.2010.00873.x)
	* “A society which measures man's [sic] worth in terms of volume of publications accumulated is no less sick than one which measures his worth in terms of dollars amassed”
	* "These impact factors pursue a circular logic—measuring the impact of academic publication activities on academic publication activities"
* [Measuring the Impacts of Science: Beyond the Economic Dimension](http://www.csiic.ca/pdf/godin_dore_impacts.pdf)
	* "There is huge demand for quantitative studies and indicators on the impact of science"
	* "For over fifty years, governments have funded research and development (R & D) because of the impact (or outcome) it has - or we think it has - on society"
	* "Most studies and indicators are concerned with economic impact"
* Usage Factors (download numbers)
	* See above - [Soft Peer Review - Taraborelli](http://discovery.ucl.ac.uk/8279/1/8279.pdf)
	* "[Harnad, cited by McKiernan; 2004](http://www.public.iastate.edu/~gerrymck/DraftFive.htm) observes that usage-based metrics are increasingly perceived by the scientific community as a necessary complement to traditional peer review as an indicator of scientific significance"
		* "This measure is noisy [in that] it can be inflated by automated web-crawlers, short-changed by intermediate caches, abused by deliberate self hits from authors, and undiscriminating between nonspecific site browsing and item-specific reading) (...), [but] seems to have some signal-value too, partly correlated with and partly independent of citation impact."
	* "Usage-based metrics, as a way to complement traditional peer review, are perceived as a major need by several actors (authors, librarians, publishers) in the scientific communication system"
* [Soft Peer Review - Taraborelli](http://discovery.ucl.ac.uk/8279/1/8279.pdf)
	* "Taken at the individual level, these metadata are hardly of any interest, but at a large scale metrics based on these metadata are likely to outperform more traditional evaluation processes in terms of coverage, speed and efficiency"
	* "There is no guarantee that bookmarking is spam-free, and social bookmarking immune to self-promotion gaming, but there are several reasons to believe it is far more reliable as a proxy than mere usage data"
		* "A bookmark indicates a single action by a user whereas in the general case there is no way to understand how many hits are generated by different users or by the same user visiting the same resource several times"
* [Article Level Metrics and the Evolution of Scientific Impact](http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1000242)
	* Usage factors (download numbers)
		* "These statistics may not be completely accurate but they are consistent, comparable, and considered sufficiently immune to cheating to be the basis for a billion dollar Web advertising industry"
	* "Each [altmetric] has its own advantages and problems"
	* "The [alt]metrics are also useful in different contexts"
	* "Our only options are to publish less or to filter more effectively, and any response that favours publishing less doesn't make sense, either logistically, financially, or ethically"
* [Not Information Overload, But Filter Failure; Shirky](http://www.web2expo.com/webexny2008/public/schedule/detail/4817)
	* Put altmetrics in this context - overload of scholarly information filtered by altmetrics and traditional metrics, where applicable
* [Clickstream Data Yields High Resolution Maps of Science; Bollen et al](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0004803)
	* "[There is a] distinction between citing behaviour and online information seeking behaviour"
* [Scientometrics 2.0; Priem & Hemminger](http://firstmonday.org/ojs/index.php/fm/article/view/2874/2570)
	* "[Altmetrics] take advantage of the pervasiveness and importance of new tools (Web pages, search engines, e–journals) to inform broader, faster, and more open metrics of impact."
	* "Despite the success of collaborative recommendation in e–commerce applications like Amazon.com and Netflix, and the existence of promising academic prototypes, automated recommending for journal articles has never caught on"
		* Cites [Don’t Look Stupid: Avoiding Pitfalls when Recommending Research Papers; McNee; 2006](http://dl.acm.org/citation.cfm?id=1180903)
* [How and why scholars cite on Twitter](http://onlinelibrary.wiley.com/doi/10.1002/meet.14504701201/full)
	* Cites [Understanding how Twitter is used to spread scientific messages; Letierce; 2009](http://journal.webscience.org/314/)
		* "The professional impact of Twitter may be particularly pronounced for scholars"
	* "Sharing information is a central component of [researchers] work"
* [Studying Scientific Discourse on the Web Using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010](http://journal.webscience.org/308/)
	* "Scientific blogs (and all other types of blogs) have major strengths, in that they have the ability to provide instantaneous commentary on a subject with simultaneous feedback on their own content"
	* "Scientific blogs also have the ability to reintroduce older publications to the reader environment, where the scientific content may be out of the scope of interest of current research spheres but may still hold interest amongst involved readers"
* [Do altmetrics work? Twitter and ten other social web services; Thelwall, Haustein, Lariviere et al; 2013](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0064841)
	* Attempts to correlate altmetrics with citation counts
	* Limiting the scope of altmetrics - _could be impactful outside of science_
	* "[For blogs, Facebook and Twitter], the success rate of the altmetrics at associating with higher citation significantly exceeded the failure rate at the individual article level"
		* Where the success is defined as the altmetric matching the citation count
	* "No cases where the number of failures is lower than the number of successes and so this suggests that, given sufficient data, all the altmetrics would also show a significantly higher success than failure rate"
	* "There is clear evidence that three altmetrics (tweets, FbWalls, blogs) tend to associate with citations at the level of individual journals"
		* Is correlation with journal-level metrics a good thing? Isn't the point of altmetrics to provide an alternative to traditional journal metrics?
	* "More recent articles are more tweeted but are typically uncited"
	* "Time from publication should be considered in addition to altmetric scores when using altmetrics to rank search results"
	* "The results provide strong evidence that six of the eleven altmetrics [that were tested] (tweets, Facebook wall posts, research highlights, blog mentions, mainstream media mentions and forum posts) associate with citation counts"
	* "The coverage of all of the altmetrics, except possibly Twitter, is low (below 20% in all cases and possibly substantially below 20%)"
		* "So these altmetrics may only be useful to identify the occasional exceptional or above average article rather than as universal sources of evidence"
			* The article mentioned the fast uptake of Twitter - As more people move their work/conversations online, this would improve
			* Social media scholarly usage is low currently, but is likely to increase
	* "[Altmetrics] might be able to capture the influence of scholarly publications on a wider and different section of their readership than citation counts, which reflect only the behaviour of publishing authors"
	* "More research is needed to identify who publishes citations to academic articles in social web sites used to generate altmetrics, and why they publish them"
* [Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012](http://arxiv.org/abs/1203.4745)
	* "Social media tools are beginning to affect the research workflow"
	* This has the advantage of "exposing and fixing scholarly processes once hidden and ephemeral"
	* "The future, then, could see altmetrics and traditional bibliometrics presented together as complementary tools presenting a nuanced, multidimensional view of multiple research impacts at multiple time scales"
	* "Scientometricians and others have for decades worked to create more realistically diverse measures of research impact, painstakingly gathering diverse indicators including patents, acknowledgements, doctoral committee membership, and many others"
	* Cites [Adoption and use of Web 2.0 in scholarly communications; Procter, Williams Stewart et al; 2010](http://rsta.royalsocietypublishing.org/content/368/1926/4039.full#sec-4)
		* Web 2.0 (self-reported) usage: 13% frequent users, 45% occasional users, 39% never use
	* "10% of UK doctoral students 'use and value' Twitter for research"
	* _"Importantly, these tools do not create new types of scholarly practice so much as they facilitate existing practice"_
		* "Social reference managers like Mendeley, for example, are an extension of paper-based bibliography collections"
		* Cites [In search for a virtual settlement: An exploration of weblog community boundaries; Efimova & Hendrick; 2005](https://doc.novay.nl/dsweb/Get/Document-46041/weblog-community-boundaries.pdf)
			* "[Blogging and Twitter] facilitate the sort of informal conference chats that have long vivified the academy’s invisible colleges"
	* Cites [Prevalence and Use of Twitter Among Scholars; Priem, Costello, Dzuba; 2011](http://jasonpriem.org/self-archived/5uni-poster.png)
		* Priem, Piwowar & Hemminger say "usage of scientific terms on Twitter is modest but reveals interesting usage patterns"
		* "Scholarly Twitter use is growing"
		* Percentage of tweets that are scholarly: 15% non-faculty, 30% faculty
	* "Mendeley … have collected 161 million documents"
	* "Over 90% of Nature and Science articles are included in Mendeley collections"
	* "Evidence of scholars using [Twitter] to enrich academic conferences"
	* Cites [Scientific citations in Wikipedia; Nielsen; 2007](http://firstmonday.org/ojs/index.php/fm/article/view/1997/1872)
		* "Citations in Wikipedia correlate with those in the Journal Citation Report [from which the IF is calculated], though slightly over-citing high-impact journals"
	* Cites [Studying Scientific Discourse on the Web using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010](http://journal.webscience.org/308/2/websci10_submission_48.pdf)
		* "While discussions in scholarly blogs pull from diverse sources, they tend to focus on articles published in highly-cited journals"
	* "Nearly every article had at least some PDF and HTML downloads"
	* "80% of articles had at least one bookmark on Mendeley while 31% had been bookmarked on CiteULike"
	* "In December 2010, 65% of PLoS research articles had received one citation in Web of Science and 50% of articles had citations from papers in PubMed Central"
		* "By December 2011, more than 92% of research articles had received at least one citation in Web of Science"
	* "Ten to twelve percent of articles had been bookmarked on Delicious, tweeted, shared on Facebook, or received a comment on the PLoS website"
		* "Note that actual tweet counts are likely higher since Twitter data was incomplete" - Used Topsy API, not Twitter firehouse
	* "About 7.5% of articles were the topic of a blog post or had received an F1000 rating"
	* "About 5% of articles had been cited on Wikipedia, Liked on Facebook, or Commented on in Facebook"
	* "Half of the papers in our dataset had at least four engaged indicators"
		* Defined "'engaged indicator' for a given paper as an indicator having a value of at least one for that paper" - a paper with 100 pdf views and a tweet has 2 engaged indicators
	* "A few power users post the lion’s share of events [bookmarks, tweets] on articles in the dataset, while the majority of users reside in the 'long tail,' contributing just a few events"
	* Found examples of "a second wave of activity [that] arrived around four months later, possibly initiated by readers alerted to the article by its first external citation"
	* "[Found a] moderately strong relationships between citation count and pdf/html download count"
	* "Mendeley and CiteULike displayed the highest correlations to Web of Science counts"
	* "Mendeley bookmark counts correlate more closely to Web of Science citations counts than expert ratings of F1000"
		* "[In an] age-restricted sample [2011 only], the correlations between Mendeley and Web of Science citations rivaled or surpassed those of Scopus, PubMed, and CrossRef citations"
	* "PLoS ONE correlations are weaker almost across the board, with negligible or zero values for altmetrics other than Mendeley. The strongest correlations are from PLoS Biology, with PLoS Pathogens tending to fall in between"
		* "This difference may be related to the number of articles a journal publishes per year. PLoS Biology is likely read in its entirety by many knowledgeable scholars, who in turn probably decide to share, cite, or discuss only the best articles"
	* "The clusters of impact patterns could be considered the 'impact flavor' of the research article"
		* These flavours of impact are more valuable than citations alone "cannot rival"
		* "The goal is not to compare flavors: one flavor is not objectively better than another"
		* "Recognizing different types of contributions might help us appreciate scholarly products for the particular needs they meet"
	* "There is no shortage of data from altmetrics sources, although different indicators vary greatly in activity"
	* "Second, altmetrics and citations track forms of impact that are distinct, but related; neither approach is able to describe the complete picture of scholarly use alone"
	* "Many altmetric indicators seem mostly orthogonal to citation"
	* "Altmetrics also have great potential for supporting personalized recommender systems"
* [New perspectives on article-level metrics: developing ways to assess research uptake and impact online; Liu & Adie; 2013](http://uksg.metapress.com/content/x65747080803n616/?genre=article&id=doi%253a10.1629%252f2048-7754.79)
	* "Placing metrics in context is far more informative [than metrics alone]"
	* "Ultimately, users must frame appropriate questions and decide what information they want the altmetrics data to provide"
	* Cites [Uptake of Journal of Ecology papers in 2012: A comparison of metrics; Gibson; 2013](http://jecologyblog.wordpress.com/2013/01/17/uptake-of-journal-of-ecology-papers-in-2012-a-comparison-of-metrics/)
		* When comparing impact factor, citation count and altmetrics, they found that "each metric reflected a different form of reader usage"
* [The Impact Factor Game; PLoS Medicine Editors; 2006](http://dx.plos.org/10.1371/journal.pmed.0030291)
	* "Magazine sections, ... “add value” to the research articles by interpreting them for a wider audience"
		* Something altmetrics can help to measure
* [Consuming Article-Level Metrics: Observations and Lessons; Chamberlain; 2013](http://www.niso.org/publications/isq/2013/v25no2/chamberlain/)
	* "Article-level metrics have many advantages over the JIF"
		* "Openness ... if data sources are open, conclusions based on article-level metrics can be verified by others and tools can be built on top of the article-level metrics"
		* "Speed ... article-level metrics are nearly real-time metrics of scholarly impact"
		* "Diversity of sources ... far more than just citations ... variety of domains, including discussion by the media, discussion by the public, and importance to colleagues"
* [Altmetrics: Rethinking the Way We Measure; Galligan & Dyas-Correia; 2013](http://www.sciencedirect.com/science/article/pii/S009879131300004X)
	* Altmetrics help to ensure that "research outcomes are disseminated effectively"
* [Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact; Eysenbach; 2011](http://www.jmir.org/2011/4/e123/)
	* "Social media activity either increases citations or reflects the underlying qualities of the article that also predict citations, but the true use of these metrics is to measure the distinct concept of social impact"
* [Institutional Altmetrics and Academic Libraries; Roemer & Borchardt; 2013](http://www.niso.org/publications/isq/2013/v25no2/roemer/)
	* "It has frequently been noted by both librarians and information scientists that researchers in STEM disciplines tend to emphasize the production and consumption of journal articles more heavily than scholars in the humanities or social sciences, for whom book–length works and monographs are also highly valued"
		* "Consequently, faculty in the humanities and social sciences have predominantly based their impact narratives on qualitative indicators"
		* Possible that altmetrics might offer quantative measures, at least in terms of wider (social) impact
