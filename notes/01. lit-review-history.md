# Final Year Project
### Lit Review

__History of bibliometrics__

* Journal Impact Factor
	* > Journal X's 2005 impact factor = Citations in 2005 (in journals indexed by Thomson Scientific) to all articles published by Journal X in 2003–2004 divided by Number of articles deemed to be “citable” by Thomson Scientific that were published in Journal X in 2003–2004 <cite>(The Impact Factor Game; The PLoS Medicine Editors; 2006)</cite>
	* Many cite Garfield's 1955 paper "Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas" as the original conception
	* Calculated by Thompson Reuters
	* Journals listed in the _Journal Citation Reports_
		* Not just JIF - other metrics too
	* [Article Level Metrics and the Evolution of Scientific Impact](http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1000242)
		* "Most scientists … will point to the Thomson ISI Journal Impact Factor as an external and 'objective' measure for ranking the impact of specific journals and the individual articles within them"
	* [Fuyuno & Cyranoski, 2006](http://www.nature.com/nature/journal/v441/n7095/full/441792b.html)
		* "[Impact Factor] is now often used to establish the value of the articles published in those journals, and by extension the quality of individual scientists’ work"
* [Show Me The Data; Rossner, Van Epps & Hill; 2007](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2140038/?tool=pubmed)
	* "[The impact factor has] a strong influence on the scientific community, affecting decisions on where to publish, whom to promote or hire, the success of grant applications, and even salary bonuses"
* [Nefarious Numbers; Arnold & Fowler; 2010](http://arxiv.org/abs/1010.0278)
	* "The impact factor has been widely adopted as a proxy for journal quality"
	* "It is used by libraries to guide purchase and renewal decisions, by researchers deciding where to publish and what to read, by tenure and promotion committees laboring under the assumption that publication in a higher impact factor journal represents better work, and by editors and publishers as a means to evaluate and promote their journals"
* [It is time to find a better way to assess the scientific literature; The PLoS Medicine Editors; 2006](http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0030291)
	* "In some countries, government funding of entire institutions is dependent on the number of publications in journals with high impact factors"
* [The History and Meaning of the Journal Impact Factor; Garfield; 2006](http://garfield.library.upenn.edu/papers/jifchicago2005.pdf)
	* "The term “impact factor” has gradually evolved, especially in Europe, to describe both journal and author impact"
* [Looking for landmarks: the role of expert review and bibliometric analysis in evaluating scientific publication outputs; Allen, Jones & Dolby; 2009](http://dx.plos.org/10.1371/journal.pone.0005910)
	* "[The impact factor's place as a key indicator of research progression] provides much of the rationale for the move to replace the UK’s Research Assessment Exercise (RAE) with a more metric-based successor"
* [The Assessment of Science: The Relative Merits of Post-Publication Review, the Impact Factor, and the Number of Citations; Eyre-Walker & Stoletzki; 2013](http://www.plosbiology.org/article/info%253Adoi%252F10.1371%252Fjournal.pbio.1001675)
	* "Several countries, including the United Kingdom, Canada, and Australia, attempt to assess the merit of the research being produced by scientists and universities and then allocate funds according to performance"
	* "In the United Kingdom, this process was known until recently as the Research Assessment Exercise (RAE); it has now been rebranded the Research Excellence Framework (REF)"
	* "The RAE and similar procedures are time consuming and expensive. The last RAE, conducted in 2008, cost the British government £12 million to perform, and universities an additional £47 million to prepare their submissions"
		* "This has led to the suggestion that it might be better to measure the merit of science using bibliometric methods, either by rating the merit of a paper by the IF of the journal in which it is published, or directly through the number of citations a paper receives"
* [How to get good science; Colquhoun; 2008](http://www.dcscience.net/?p=182)
	* "Academics, like everyone else, are expected to do a good job"
	* "The problem is that it is very hard to measure the value of their output"
	* Describes the use of the impact factor as an example of Goodhart's law
		* "When a measure becomes a target, it ceases to be a good measure"

__Failings of traditional metrics__

* [Altmetrics Manifesto](http://altmetrics.org/manifesto/)
	* "As the volume of academic literature explodes, scholars rely on filters to select the most relevant and significant sources from the rest"
	* "Scholarship’s three main filters for importance are failing"
		* Peer-review (out of scope?)
		* Citation counts:
			* "useful, but not sufficient"
			* "Metrics like the h-index are even slower than peer-review: a work’s first citation can take years"
			* "Neglect impact outside the academy"
			* "Ignore the context and reasons for citation"
		* JIF:
			* "often incorrectly used to assess the impact of individual articles"
			* "Exact details of the JIF are a trade secret"
				* [Show Me The Data](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2140038/?tool=pubmed)
			* Significant gaming is relatively easy
				* [Nefarious Numbers](http://arxiv.org/abs/1010.0278)
				* [It is time to find a better way to assess the scientific literature](http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0030291)
* [Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009](http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1000242)
	* "Over 800,000 new articles appeared in PubMed in 2008"
	* "The total is now over 19 million"
	* "It [is] impossible for any scientist to read every paper relevant to their research, and a difficult choice has to be made about which papers to read"
	* Impact factor
		* "The impact factor … cannot escape an even more fundamental problem: it is simply not designed to capture qualities of individual papers"
	* Citation metrics - a form of article-level metrics (as opposed to journal-level metrics)
		* "The biggest problem [with citation metrics], though, is the time-delay inherent in citations"
			* "The first citations to a paper will appear — at the earliest — months after it is first available; far too late to be useful in the days and weeks after it is published"
	* Comments
		* "Commenting in the scientific community simply hasn't worked, at least not generally"
		* "Researchers are unsure how to behave in this new space"
* [Show Me The Data; Rossner, Van Epps & Hill; 2007](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2140038/)
	* "It is mathematically problematic … with around 80% of a journal impact factor attributable to around 20% of the papers, even for journals like Nature" (cited by [Article Level Metrics and the Evolution of Scientific Impact](http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1000242))
	* "Members of the [scientific] community seem to have little understanding of how impact factors are determined, and, to our knowledge, no one has independently audited the underlying data to validate their reliability"
	* Cites [Hwang’s Retracted Publication Still Contributes to Science’s Impact Factor; Liu; 2007](http://im1.biz/albums/userpics/10001/SE2007V2N2A2_Hwang.pdf)
		* "Citations to retracted articles are counted in the impact factor calculation"
		* "Woo Suk Hwang's stem cell papers in Science from 2004 and 2005, both subsequently retracted, have been cited a total of 419 times"
	* "The impact factor calculation is a mean, [so] it can be badly skewed by a 'blockbuster' paper"
	* "When we asked Thomson Scientific if they would consider providing a median calculation in addition to the mean they already publish, they replied:"
		* > It's an interesting suggestion… The median… would typically be much lower than the mean. There are other statistical measures to describe the nature of the citation frequency distribution skewness, but the median is probably not the right choice
	* "There are ways of playing the impact factor game"
		* "Review articles typically garner many citations, as do genome or other 'data-heavy' articles"
	* Found "numerous incorrect article-type designations" in Thomson-Reuters' data determining the impact factor
	* Also found that "the total number of citations for each journal was substantially fewer than the number published on the Thomson Scientific, Journal Citation Reports (JCR) website", which would lead to the impact factor being affected
	* _"It became clear that Thomson Scientific could not or (for some as yet unexplained reason) would not sell us the data used to calculate their published impact factor"_
		* "If an author is unable to produce original data to verify a figure in one of our papers, we revoke the acceptance of the paper"
	* "Just as scientists would not accept the findings in a scientific paper without seeing the primary data, so should they not rely on Thomson Scientific's impact factor, which is based on hidden data"
* [Problems of citation analysis: A study of uncited and seldom-cited influences; MacRoberts & MacRoberts; 2009](http://onlinelibrary.wiley.com/doi/10.1002/asi.21228/full)
	* "Most of the work used is not cited"
	* "Theoretical/analytical articles that are predominantly published in Thomson Reuters-monitored journals are cited, and data articles that provide the basis of the theoretical/analytical work and that are seldom published in Thomson Reuters-monitored journals are not cited"
		* What does this mean? Only _articles_ published in Thompson-Reuters journals are cited, and the body of work that supports the articles are not cited
* [The Impact Factor: A Tool from a Bygone Era? Anderson; 2009](http://scholarlykitchen.sspnet.org/2009/06/29/is-the-impact-factor-from-a-bygone-era/)
	* "Citation is occurring in new ways, and scientific thinking is not always propagated via the published scientific article"
* [Journal status; Bollen, Rodriquez & Van de Sompel; 2006](http://link.springer.com/article/10.1007%2Fs11192-006-0176-z)
	* "[The impact factor] seems to only represent the popularity factor of status, not its prestige factor"
		* Popularity: "total number of endorsements"
		* Prestige: "the prestige of endorsing actors"
	* An example of an algorithm that considers prestige: Google's PageRank
		* "By taking into account both the popularity and the prestige factor of status, Google has been able to avoid assigning high ranks to popular but otherwise irrelevant web pages"
	* Found that their PageRank-like algorithm "strongly-overlapped" with the impact factor, but it "revealed significant and meaningful discrepancies"
	* They think that a "meaningful difference" between their PageRank-like algorithm and the impact factor is "a reason to seriously contemplate the use of a variety of journal status metrics instead of just one"
* [Nefarious Numbers; Arnold & Fowler; 2010](http://arxiv.org/abs/1010.0278)
	* "The allure of the impact factor as a single, readily available number - not requiring complex judgments or expert input, but purporting to represent journal quality - has proven irresistible to many"
	* "There have also been widespread complaints by authors of manuscripts under review, who were asked or required by editors to cite other papers from the journal"
		* "This practice borders on extortion"
	* "The cumulative result of the design flaws and manipulation is that impact factor gives a very inaccurate view of journal quality"
* [It is time to find a better way to assess the scientific literature; The PLoS Medicine Editors; 2006](http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0030291)
	* "A journal's impact factor says nothing at all about how well read and discussed the journal is outside the core scientific community"
		* "[For a journal] which strives to make … open-access content reach the widest possible audience … impact factor is a poor measure of overall impact"
	* "Thomson Scientific acknowledges that the impact factor has grown beyond its control and is being used in many inappropriate ways"
	* "Editors may decrease the denominator by attempting to have whole article types removed from it"
		* "By making such articles superficially less substantial, such as by forcing authors to cut down on the number of references or removing abstracts"
		* "By decreasing the number of research articles published"
	* "The impact factor depends crucially on which article types Thomson Scientific deems as 'citable' — the fewer, the better"
		* "[Thomson Scientific] refuses to make public its process for choosing “citable” article types"
	* "During discussions with Thomson Scientific … it became clear that the process of determining a journal's impact factor is unscientific and arbitrary"
	* "Thomson Scientific has no explicit process for deciding which articles other than original research articles it deems as citable"
	* "Magazine sections [articles aimed at a layman audience] … not only “add value” to the research articles by interpreting them for a wider audience but have other vital roles: they may help to set agendas"
		* "Such articles will rarely be cited in indexed journals, but may be influential"
	* "It is not clear whether Thomson Scientific could measure ... individual article citations accurately"
* [Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012](http://arxiv.org/abs/1203.4745)
	* "Citation tracking has never been able to follow the less visible — but often more important — threads of invisible colleges, woven through personal connections and informal communications"
	* "The 'citation latency' of even high-impact articles may be '1-2 years or even longer'"
	* Citation counts do not measure "other important products such as datasets"
	* Cites [Nature web focus: Access to the literature: the debate continues; King & Tenopir; 2004](http://www.nature.com/nature/focus/accessdebate/26.html)
		* "Only about 15 to 20% of scientists in the United States have authored a refereed article"
	* "These new approaches, however, have not completely filled the gap between citation metrics and real-world impact"
	* "Webometric approaches that rely on search engines are fundamentally limited by terms of use restrictions on automated mining of results"
	* Cites [Applying social bookmarking data to evaluate journal usage; Haustein & Siebenlist; 2011](http://www.sciencedirect.com/science/article/pii/S1751157711000393)
		* "Usage statistics based on worldwide article downloads are not available"
		* "Global usage data are generally wrapped in mystery by the publishers"
* [The Assessment of Science: The Relative Merits of Post-Publication Review, the Impact Factor, and the Number of Citations; Eyre-Walker & Stoletzki; 2013](http://www.plosbiology.org/article/info%253Adoi%252F10.1371%252Fjournal.pbio.1001675)
	* Cites [Looking for landmarks: the role of expert review and bibliometric analysis in evaluating scientific publication outputs; Allen, Jones & Dolby; 2009](http://dx.plos.org/10.1371/journal.pone.0005910)
		* "Level of agreement between experts was low"
		* "[Expert] score was moderately correlated to the number of citations the paper had obtained 3 years after publication"
		* "However, … also found that the assessor score was more strongly correlated to the IF of the journal in which the paper was published than to the number of citations"
			* "[It is] therefore possible that the correlation … was a consequence of assessors rating papers in high profile journals more highly, rather than an ability of assessors to judge the intrinsic merit or likely impact of a paper"
		* "[The impact factor's place as a key indicator of research progression] provides much of the rationale for the move to replace the UK’s Research Assessment Exercise (RAE) with a more metric-based successor"
	* "[Expert] assessors give higher scores to papers in high IF journals (or underrate the science in low IF journals), independent of their merit"
	* "[Expert] assessor score is more strongly dependent upon the IF than the number of citations"
	* "If we control for IF, we find that the correlation between assessor score and the number of citations becomes weak"
	* _Criticism of the REF_
		* "Both subjective review and the number of citations are very error prone measures of merit, so it seems likely that [the REF] will also be extremely error prone, particularly given the volume of assessments that need to be made"
* [How to Use Citation Analysis for Faculty Evaluations, and When Is It Relevant?; Garfield; 1983](http://garfield.library.upenn.edu/essays/v6p354y1983.pdf)
	* Written by Garfield (see above - the inventor of the original impact factor)
	* "Although journal assessments are important, evaluation of faculty is a much more important exercise that affects individual careers. Impact numbers should not be used as surrogates except in unusual circumstances."
* [Are Alternative Metrics Still Alternative?; Buschman & Michalek; 2013](http://www.asis.org/Bulletin/Apr-13/AprMay13_Buschman_Michalek.html)
	* "The current promotion system, however, discourages publishing research with negative results"
	* "Many scientists are using blogs to show more details of their research including negative results"
		* "Being able to measure the impact of this output in non-traditional venues and formats will encourage scientists to share more details of their research."
	* "Descriptions of methods and settings are increasingly being posted [on blogs]"
	* Draws analogy to Horatio Nelson Jackson who was the first to drive a car across the US
		* "He did not wait for highways to be built"
		* "In the same way the popularity of the car created the demand for better highways, the availability of more complete impact metrics for research will surely change the current system"
* [Consuming Article-Level Metrics: Observations and Lessons; Chamberlain; 2013](http://www.niso.org/publications/isq/2013/v25no2/chamberlain/)
	* Cites [Impact factor distortions; Alberts; 2013](http://www.sciencemag.org/content/340/6134/787)
		* There is a "growing list of scientists and societies that would like to stop the use of the JIF in judging work of scientists"
* [Altmetrics: Rethinking the Way We Measure; Galligan & Dyas-Correia; 2013](http://www.sciencedirect.com/science/article/pii/S009879131300004X)
	* "The final calculations for impact factors are largely unknown, and the underlying data are not subject to independent audit"
* [Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact; Eysenbach; 2011](http://www.jmir.org/2011/4/e123/)
	* "[Traditional metrics] are often available only in proprietary databases; thus, these metrics are not necessarily transparent or _reproducible_"
