# What is altmetrics?

As alternative to traditional bibliometrics, the concept of altmetrics was developed in the late 2000s. Although there are many different definitions, altmetrics as a term has evolved over the years to an umbrella term for metrics that measure impact of web-based scholarly communications both qualitatively and quantitatively [Altmetrics Manifesto; Priem, Taraborelli, Groth &  Neylon; 2010]  [Thoughts from the Fishbowl: PLOS ALM Workshop 2013; Liu; 2003]  [As Scholarship Goes Digital, Academics Seek New Ways to Measure Their Impact; Howard; 2012]. Altmetrics, in contrast to the _journal_ impact factor, are primarily measured at the article level, although it has been suggested by some that altmetrics should also measure impact outside of the traditional article [Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009]  [@McDawg altmetrics could cover person, uni or journal level stats, non-trad contributions & items. ALMs a subset, fixes article as focus; Adie; 2013]. Article level metrics is a term somewhat favoured initially by those looking for an alternative to the impact factor. However article level metrics have started to be incorporated under the banner of altmetrics [I like the term #articlelevelmetrics, but it fails to imply *diversity* of measures. Lately, I'm liking #altmetrics; Priem; 2010].

Metrics that have been included in altmetrics in the past include article views and downloads, scholarly tweets, bookmarks on bookmarking services like Delicious, saves on social referencing services like Mendeley and even traditional citations [Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012]. There is a consensus that altmetrics are centre around mining web-based resources [As Scholarship Goes Digital, Academics Seek New Ways to Measure Their Impact; Howard; 2012]. There exists no definitive list of metric sources, and some disagree on the validity of specifics, but there is some movement towards standardising metrics and how they are measured [Do altmetrics work? Twitter and ten other social web services; Thelwall, Haustein, Lariviere et al; 2013]  [Altmetrics in Evolution: Defining and Redefining the Ontology of Article-Level Metrics; Liu & Fenner; 2013].

Kaitlin Thaney, Director of the Mozilla Science Lab, likened altmetrics to tracking "a researcher's footprints in the community" [Thoughts from the Fishbowl: PLOS ALM Workshop 2013; Liu; 2003]. A good metaphor for describing how altmetrics attempts to capture impact from a wide range of sources. It is this wide range of sources, and their relative ease of access that allows not only the measurement of scholarly communications, but also of the wider general public [Scientometrics 2.0; Priem & Hemminger; 2010]. Although the webometrics measurements have mined the web for impact data before, they have not updated for the rise of the social web, or Web 2.0. This new form of interaction on the web, enables a much larger audience to access publishing tools on the web. Users are using these tools to chat, discuss and share interesting links, activities that could potentially generate impact for researchers by measuring how far their research is spread [Altmetrics: impact, landscape, their value to the library; Galligan; 2012]. Priem & Hemminger state that "[the] emergence of 'Web 2.0' presents a new window through which to view the impact of scholarship" [Scientometrics 2.0; Priem & Hemminger; 2010].

The web's usage by scholars to communicate has been growing, and looks to continue into the future [Altmetrics Manifesto; Priem, Taraborelli, Groth & Neylon; 2010]. This usage comes in many forms, with studies finding usage on social media services, Twitter and Facebook [How and why people Twitter: the role that micro-blogging plays in informal communication at work; Zhao and Rosson; 2009], on blogs [Studying Scientific Discourse on the Web Using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010], on bookmarking services, such as Delicious [Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012], and social reference managers, such as Mendeley [The spread of scientific information: insights from the web usage statistics in PLoS article-level metrics; Yan & Gerstein; 2011]. Fenner found that 93% of PLOS Biology research articles published since June 2012 have been discussed on Twitter, and 63% mentioned on Facebook [What Can Article-Level Metrics Do for You?; Fenner; 2013].

A key feature of altmetrics is the diversity of data sources - as discussed, the impact factor's simplistic model does not adequately measure the complex system that is scientific impact [What Can Article-Level Metrics Do for You?; Fenner; 2013]. There is no definitive list of data sources that altmetrics practitioners have standardised around, and different studies have used different data sources, and different implementations of retrieving the sources [NISO ALM standardization workshop; Mulvany; 2013]. Below is a list of data sources that previous studies have used.

### Twitter
Twitter as a service has grown rapidly in recent years [Scientometrics 2.0; Priem & Hemminger; 2010]. Several studies have looked at the scholarly usage of Twitter, finding that it has been used for a variety of academic purposes [How and why scholars cite on Twitter; Zhao & Rosson; 2009]  [Understanding how Twitter is used to spread scientific messages; Letierce; 2009]  [How the scientific community reacts to newly submitted preprints: article downloads, Twitter mentions, and citations; Shuai, Pepe & Bollen; 2012]  [Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact; Eysenbach; 2011]. Zhao & Rosson found that 6% of sampled tweets contained a link to a paper, of which, 52% directly linked to a paper and 48% linked to a third-party which then linked to a paper [How and why people Twitter: the role that micro-blogging plays in informal communication at work; Zhao and Rosson; 2009].

Eysenbach studied Twitter usage after article publication, finding that "the top 20% of the tweet authors accounted for 63.4% of all tweetations", which roughly follows the 80/20 rule. Finally, he found that tweets can predict highly cited articles within the first 3 days of article publication, and concludes that tweets could be used as an alternative to the impact factor. [Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact; Eysenbach; 2011].

Shuai, Pepe & Bollen studied the usage of Twitter by scholars over time after a new paper has been released. They found that most Twitter mentions of a paper occur one day after publication, and that is the only day the article is mentioned on Twitter. Articles are quickly passed around with little in-depth discussion. They conclude that either early Twitter mentions drive greater download numbers, or alternatively, inherently higher quality articles generate high early Twitter mentions [How the scientific community reacts to newly submitted preprints: article downloads, Twitter mentions, and citations; Shuai, Pepe & Bollen; 2012]. Eysenbach found similar usage - a majority of tweets related to a paper (Eysenbach terms them "tweetations") were sent on the day of article publication, and after 30 days, tweets would move into "sporadic tweetation phase" [Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact; Eysenbach; 2011].

Desai, Shariff, Shariff et al. found that tweets related to an academic conference had more positive sentiment scores if they were classified as opinion tweets. They also found that "a positive sentiment score is not a prerequisite for message amplification" [Tweeting the meeting: an in-depth analysis of Twitter activity at Kidney Week 2011; Desai, Shariff, Shariff et al; 2012].

Zhao and Rosson interviewed scholarly Twitter users and found that they "typically follow people both in and out of their particular subfields", which gives conversations a "more interdisciplinary perspective". This broad scope of conversations is interesting to altmetric practitioners, as it affords an opportunity to measure more impact that is more wide ranging. Zhao and Rosson additionally found that participants "emphasized that they saw citing on Twitter as part of a dynamic, ongoing conversation" [How and why scholars cite on Twitter; Zhao & Rosson; 2009].

### Blogs
Blogging as a medium started in the late 1990s, but grew more popular in the mid-2000s, unlike most of the tools discussed here, it is not closely associated with a "name-brand" service, perhaps reflecting on the maturity of the medium. This maturity combined with the ease of publication that is possibly the reason for blogging's popularity among scholars, with Priem & Hemminger commenting that the literature is much too large to review in its entirety [Scientometrics 2.0; Priem & Hemminger; 2010]. Scholars use blogs for a variety of reasons, including sharing content, expressing opinions and interacting with others both inside and outside of the author's discipline [Research blogs and the discussion of scholarly information; Shema, Bar-Ilan, Thelwall; 2012]. Groth and Gurney argue that scholarly communication is not separated between articles and blogs, but intertwined, with blogs increasingly referring to traditional publications [Studying Scientific Discourse on the Web Using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010]  [Research blogs and the discussion of scholarly information; Shema, Bar-Ilan, Thelwall; 2012]. Blogs are examples of participatory journalism, with scientific-focussed blogs addressing topics published in journals but also extending to the general public, often in a formal setting. Most posts describe the implications of science [Studying Scientific Discourse on the Web Using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010]  [Scientometrics 2.0; Priem & Hemminger; 2010]. This variety makes blogs useful to altmetrics practitioners, as they allow measurement of different forms of impact, both scholarly and non-scholarly.

The demographics of scholarly bloggers are skewed towards males affiliated with academic institutions, according to Shema, Bar-Ilan and Thelwall's sample. They found that 67% of the sample reporting to be male, 18% female, 5% male-male co-authors, 4% male-female co-authors, and 6% unknown. They also found 59% were either students or researchers in an academic institute, and less than a third were not affiliated with an institute. The study also looked at blogger's Twitter accounts, finding that 72% had active Twitter accounts, and they many followed fellow bloggers. They concluding that there is a core of quite well connected  scholarly bloggers who are "information disseminators" [Research blogs and the discussion of scholarly information; Shema, Bar-Ilan, Thelwall; 2012].

Unlike Twitter, Facebook and other social media tools, blogging's culture of linking to primary sources is strong, and is similar to academics culture of citations. This makes links on blogs a strong indicator of impact [Scientometrics 2.0; Priem & Hemminger; 2010]. Blogs tend to reference "high quality science", with 70.5% of the publications references were in journals with high impact factors [Studying Scientific Discourse on the Web Using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010].

Priem & Hemminger report that mining blogs to spot trends has been an area of active research, with some blog trend detection services created [BlogPulse: Automated trend discovery for weblogs; Glance et al; 2004]  [BlogScope: A System for Online Analysis of High Volume Text Streams; Bansal & Koudas; 2007]. These systems scrape the blogosphere to spot emerging trends, which can then be mapped to scholarly impact [Scientometrics 2.0; Priem & Hemminger; 2010].

As mentioned above, the blogging literature is too large and disparate to measure in it's entirety so many altmetrics tools use speciality blogging aggregators, such as ResearchBlogging.com, to take a sample of scholarly focussed blogs. These aggregators allow academic blogs to submit new posts and an editor ensures that posts follow guidelines and are of appropriate quality [Research blogs and the discussion of scholarly information; Shema, Bar-Ilan, Thelwall; 2012].

Groth & Gurney report that mining blogs provide more immediate scientific discourse, similar to other altmetrics data sources. They also find that newer articles get more blog post page views, but older articles will be referenced and put into context by current articles [Studying Scientific Discourse on the Web Using Bibliometrics: A Chemistry Blogging Case Study; Groth & Gurney; 2010].

### Social Reference Managers
Social reference managers are a fairly new category, dominated by Mendeley, started in 2009 (_?????_). These services allow researchers to collect papers they find interesting and take notes on them, in an easily accessible place. This is similar to older services, such as Endnote, however they add the ability for a researcher to view a feed of the papers other researchers have saved [Social Signals Reflect Academic Impact: What it Means When a Scholar Adds a Paper to Mendeley; Gunn; 2013]  [Adoption and use of Web 2.0 in scholarly communications; Procter, Williams Stewart et al; 2010]. "By broadcasting what papers they think are important, researchers are directly influencing the research community's choice of reading and discussion material". In addition, the researcher has the ability to add tags, comments or ratings when saving papers [Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009].

Mendeley is the most popular of these services, with Gunn reporting broad adoption in the life scientists, chemistry, maths and computer science. Mendeley stores 420 million documents, with half a million new documents added a day. A high percentage of recently released articles are represented in the Mendeley collection of documents [Social Signals Reflect Academic Impact: What it Means When a Scholar Adds a Paper to Mendeley; Gunn; 2013]  [Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact; Priem, Piwowar, Hemminger; 2012]. Despite it's dominance, Mendeley is not the only service in this market, with some competitors such as Zotero [Scientometrics 2.0; Priem & Hemminger; 2010].

Mendeley offers a public API, or application programming interface, that allows altmetrics practitioners to directly access usage data. The API provides an endpoint for articles that returns the number of users who saved the article. Articles can be referencing using PubMed ID, arXiv ID, DOI ID, ISBN number or an ISSN. The API is updated approximately daily with the latest data [Social Signals Reflect Academic Impact: What it Means When a Scholar Adds a Paper to Mendeley; Gunn; 2013] _????? Add reference to Mendeley API ??????_.

As data from social reference managers is generated by scholars its value to altmetrics is high. Practitioners have a high degree of confidence that impact generated from social reference managers originates from trained scholars. In addition, tagging and rating metadata is valuable, providing more insight into scholarly impact. Neylon and Wu even speculate that reference managers might add future functionality to able to "track the amount of time users spend viewing papers within their interface" [Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009].

### Social bookmarking
Bookmarking services are very similar to reference managers, in that they allow users to save links and view other's saved links, perhaps in a feed of links. Both scholars and non-scholars use social bookmarking services. Bookmarking services also allow users to tag and comment on links, giving valuable metadata [Article Level Metrics and the Evolution of Scientific Impact; Neylon & Wu; 2009].

The major difference between reference managers and bookmarking services is the likelihood that impact is from scholarly sources. Bookmarking services tend to be aimed at the general public, whereas reference managers are focussed on scholars. This difference has advantages for altmetrics practitioners, as allows measurement of wider impact.

There are several social bookmarking services, including the widely used Delicious, and the more scholarly focussed CiteULike. Priem and Hemminger describe bookmarking as "[maybe] the best–developed scholarly Web 2.0 application", although business focus in this area has dropped off recently [Scientometrics 2.0; Priem & Hemminger; 2010].

There are some early discussions in this area, with some consensus around data source types. However not all feel that standardisation is warranted yet, and will lead to calcification.

* Metrics measured by altmetrics
	* Descriptions of services that generate metrics & their usage by scholars
		* User demographics
		* Twitter
			* API
		* Social bookmarking
			* Delicious
			* CiteULike
			* Connotea (?)
			* "An item in an online bookmarking system is described by a list of tags, ratings, annotations compiled by the user when filing the item in his or her library"
			* Similar to social reference managers
				* User saves links (a form of citation)
				* Others can view links/receive feed of other's links
				* Can attach metadata
			* Doesn't imply strong academic impact (of social reference managers)
		* Github
			* Code
		* Data
			* figshare
			* Dryad
	* ALMs
		* Views/downloads
		* Traditional citations (?)
* Categories of metrics
	* "We moved from an emphasis on the data source itself to the underlying activity captured by the data source"
		* By categorising metrics, we can capture the underlying intent in the metric
	* Concept of increasing engagement
		* Citing shows more engagement than a view/download
		* "When readers first see an interesting article, their response is often to view or download it. By contrast, a citation may be one of the last outcomes of their interest, occurring only about 1 in 300 times a PLOS paper is viewed online"
			* Doesn't however cover levels of access to more engaged metrics - i.e. not everyone can write a research paper in which to cite!
				* Could split, like ImpactStory does - into general public impact, and scholarly impact
					* "We suggest categorizing metrics along two axis: engagement type and audience"
	* Views/Downloads
	* Saved/Bookmarked
	* Shared/Recommended
	* Discussed
	* Formal citation
	* Hierarchy of metrics
		* Primary metrics: "includes the raw counts of activity captured by each source"
		* Secondary metrics: "comprised of descriptive statistics that give context to the primary metrics (e.g., article view to PDF download ratio and average usage of similar papers)"
* Existing altmetrics services
* Speed of altmetrics
	* Finds that "scientific discourse on the Web is more immediate" and that posts that discuss newer articles get more page views
* Timeline of usage
	* "The spread of a paper will then be reflected at the level of web usage statistics, in particular, the number of HTML views"
	* "On average, the older a paper is, the less attention it receives"
	* "In particular, from the first month to the second month, the decay is rapid, while later on the decay goes slower"
	* "After a scientist has accessed a paper (and hopefully read it as well), he/she might spread the information of the paper to his friends, colleagues or students. The information would then be further spread via a cascade of social interactions"
		* Follows simple stochastic model